# ‡¶™‡¶æ‡¶á‡¶•‡¶®‡ßá ‡¶∏‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶®‡¶æ‡¶≤‡¶æ‡¶á‡¶∏‡¶ø‡¶∏

‡¶è‡¶ñ‡¶æ‡¶®‡ßá  ‡¶∞‡¶ï‡¶Æ ‡¶ü‡ßÅ‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶ú‡¶® ‡¶´‡ßÅ‡¶°‡ßá‡¶∞ ‡¶∞‡¶ø‡¶≠‡¶ø‡¶â ‡¶è‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶∏‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶®‡¶æ‡¶≤‡¶æ‡¶á‡¶∏‡¶ø‡¶∏ ‡¶ï‡¶∞‡¶¨

1. VADER (Valence Aware Dictionary and sEntiment Reasoner) - Bag of words approach
2. Roberta Pretrained Model from ü§ó

# ‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá‡¶á ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶á‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø 


```python
#‡¶°‡¶æ‡¶ü‡¶æ ‡¶´‡ßç‡¶∞‡ßá‡¶Æ‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶æ‡¶®‡ßç‡¶°‡¶æ‡¶∏ ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø
import pandas as pd
# ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶æ‡¶Æ‡¶™‡¶æ‡¶á 
import numpy as np
# ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶è‡¶®‡¶æ‡¶≤‡¶æ‡¶á‡¶∏‡¶ø‡¶∏ ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡¶™‡ßç‚Äå‡¶≤‡¶ü ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø 
import matplotlib.pyplot as plt
# ‡¶è‡¶ï‡¶á ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶ø‡¶¨‡¶∞‡ßç‡¶£
import seaborn as sns

plt.style.use('ggplot')

# ‡¶°‡¶æ‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶® ‡¶è‡¶≤ ‡¶ü‡¶ø ‡¶ï‡ßá ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø
import nltk

```


```python
# ‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá‡¶á ‡¶™‡¶æ‡¶®‡ßç‡¶°‡¶æ‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶´‡ßç‡¶∞‡ßá‡¶Æ‡ßá ‡¶∏‡¶ø‡¶è‡¶∏‡¶≠‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶ü‡¶æ ‡¶á‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡ßá‡¶á
df = pd.read_csv('/mnt/Work/raw_manjaro/database/Reviews.csv')
# ‡¶°‡¶æ‡¶ü‡¶æ‡¶∞ ‡¶∂‡ßá‡¶á‡¶™ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 
print(df.shape)


df = df.head(500)
print(df.shape)
```

    (568454, 10)
    (500, 10)



```python
df.head(500)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>ProductId</th>
      <th>UserId</th>
      <th>ProfileName</th>
      <th>HelpfulnessNumerator</th>
      <th>HelpfulnessDenominator</th>
      <th>Score</th>
      <th>Time</th>
      <th>Summary</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>B001E4KFG0</td>
      <td>A3SGXH7AUHU8GW</td>
      <td>delmartian</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>1303862400</td>
      <td>Good Quality Dog Food</td>
      <td>I have bought several of the Vitality canned d...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>B00813GRG4</td>
      <td>A1D87F6ZCVE5NK</td>
      <td>dll pa</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1346976000</td>
      <td>Not as Advertised</td>
      <td>Product arrived labeled as Jumbo Salted Peanut...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>B000LQOCH0</td>
      <td>ABXLMWJIXXAIN</td>
      <td>Natalia Corres "Natalia Corres"</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1219017600</td>
      <td>"Delight" says it all</td>
      <td>This is a confection that has been around a fe...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>B000UA0QIQ</td>
      <td>A395BORC6FGVXV</td>
      <td>Karl</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
      <td>1307923200</td>
      <td>Cough Medicine</td>
      <td>If you are looking for the secret ingredient i...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>B006K2ZZ7K</td>
      <td>A1UQRSCLF8GW1T</td>
      <td>Michael D. Bigham "M. Wassir"</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1350777600</td>
      <td>Great taffy</td>
      <td>Great taffy at a great price.  There was a wid...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>495</th>
      <td>496</td>
      <td>B000G6RYNE</td>
      <td>APGAA43E3WPN7</td>
      <td>Darren</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1201392000</td>
      <td>amazing chips</td>
      <td>i rarely eat chips but i saw these and tried t...</td>
    </tr>
    <tr>
      <th>496</th>
      <td>497</td>
      <td>B000G6RYNE</td>
      <td>ABR7HU5H1KNE</td>
      <td>Keith</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1196726400</td>
      <td>Best Chip Ever</td>
      <td>This is easily the best potato chip that I hav...</td>
    </tr>
    <tr>
      <th>497</th>
      <td>498</td>
      <td>B000G6RYNE</td>
      <td>AJQD2WWJYOYFQ</td>
      <td>bubbles</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>1186617600</td>
      <td>Tangy, spicy, and sweet- oh my!</td>
      <td>Kettle Chips Spicy Thai potato chips have the ...</td>
    </tr>
    <tr>
      <th>498</th>
      <td>499</td>
      <td>B000G6RYNE</td>
      <td>A16YH487W9ZYO0</td>
      <td>Bruce G. Lindsay</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>1184198400</td>
      <td>An indulgence with a bite</td>
      <td>Okay, I should not eat potato chips, nor shoul...</td>
    </tr>
    <tr>
      <th>499</th>
      <td>500</td>
      <td>B000G6RYNE</td>
      <td>A83YQC1XOU4CS</td>
      <td>J. Baker</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1183420800</td>
      <td>The best I've had</td>
      <td>I don't write very many reviews but I have to ...</td>
    </tr>
  </tbody>
</table>
<p>500 rows √ó 10 columns</p>
</div>



## ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßá‡¶ü‡¶ü‡¶æ ‡¶ï‡ßá‡¶Æ‡¶® ‡¶¶‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∞‡¶ø‡¶≠‡¶ø‡¶â ‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡ßá ‡¶™‡ßç‡¶≤‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø

### ‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá ‡¶°‡¶æ‡¶ü‡¶æ ‡¶´‡ßç‡¶∞‡ßá‡¶Æ‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶≤‡¶ï ‡¶ï‡¶∞‡¶≤‡¶æ‡¶Æ‡•§ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶ü‡¶æ ‡¶≤‡¶ï ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
df['Score']
### ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 
value_counts()
### ‡¶è‡¶ï‡ßá ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶á‡¶®‡ßç‡¶°‡ßá‡¶ï‡ßç‡¶∏‡¶ø‡¶Ç ‡¶ï‡¶∞‡¶≤‡¶æ‡¶Æ
sort_index()
### ‡¶á‡¶®‡¶°‡ßá‡¶ï‡ßç‡¶∏ ‡¶ï‡ßá ‡¶™‡ßç‡¶≤‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 
.plot ‡¶¨‡ßç‡¶Ø‡¶æ‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶≤‡¶æ‡¶Æ


### ‡¶™‡ßç‡¶≤‡¶ü‡¶ï‡ßá ‡¶Æ‡¶°‡¶ø‡¶´‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
plot ‡¶ü‡¶æ‡¶á‡¶™ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø 
.plot(kind='bar')
,title='TITLE NAME',
figsize=(10,5)

#### ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø
ax.set_xlabel('LABEL NAME')

## ‡¶™‡ßç‡¶≤‡¶ü ‡¶∂‡ßã ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø 
plt.show()


```python
ax = df['Score'].value_counts().sort_index() \
    .plot(kind='bar',
          title='Count of Reviews by Stars',
          figsize=(15, 5))
ax.set_xlabel('Review Stars')
ax.set_ylabel('Numbers')
plt.show()
```


    
![png](output_7_0.png)
    



```python

```

## Basic NLTK

### ‡¶Ø‡ßá ‡¶°‡¶æ‡¶ü‡¶æ ‡¶®‡¶ø‡ßü‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶∏‡ßá‡¶ü‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ‡•§ ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã ‡¶è‡¶ï‡ßç‡¶ü‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ‡¶ï‡ßá ‡¶∞‡¶ø‡¶° ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶æ‡¶ü‡¶æ ‡¶´‡ßç‡¶∞‡ßá‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã ‡¶è‡¶ï‡ßç‡¶ü‡¶æ ‡¶∞‡ßã ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶õ‡¶ø


```python
example = df['Text'][50]
print(example)
```

    This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.



```python
examaple2 = df['Text'][111]
print(examaple2)
```

    I have been drinking this tea for a long time now.  I used to have to purchase it at a doctor's office because it wasn't available elsewhere.  I'm so glad that I can buy it now from Amazon.com.  I drink this tea throughout the day like other folks drink coffee.  Wonderful taste.


### ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶ï‡ßá ‡¶ü‡ßã‡¶ï‡ßá‡¶®‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø‡•§ ‡¶ü‡ßã‡¶ï‡ßá‡¶®‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ‡¶π‡¶≤‡ßã ‡¶™‡ßÅ‡¶∞‡ßã ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶õ‡ßã‡¶ü ‡¶õ‡ßã‡¶ü ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡ßá‡ßü‡¶æ


```python
tokens = nltk.word_tokenize(example)
tokens[:10]
```




    ['This', 'oatmeal', 'is', 'not', 'good', '.', 'Its', 'mushy', ',', 'soft']



### ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï‡¶ü‡¶æ ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶ï‡ßã‡¶® ‡¶™‡¶æ‡¶∞‡ßç‡¶ü‡¶∏ ‡¶Ö‡¶´ ‡¶∏‡ßç‡¶™‡¶ø‡¶ö ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø


```python
tagged = nltk.pos_tag(tokens)
tagged[:10]
```




    [('This', 'DT'),
     ('oatmeal', 'NN'),
     ('is', 'VBZ'),
     ('not', 'RB'),
     ('good', 'JJ'),
     ('.', '.'),
     ('Its', 'PRP$'),
     ('mushy', 'NN'),
     (',', ','),
     ('soft', 'JJ')]



## ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá ‡¶ï‡¶∞‡ßá ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ï‡¶∞‡ßá ‡¶®‡ßá‡ßü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ö‡¶æ‡¶ô‡ßç‡¶ï‡¶ø‡¶Ç ‡¶ï‡¶∞‡¶õ‡¶ø 


```python
entities = nltk.chunk.ne_chunk(tagged)
entities.pprint()
```

    (S
      This/DT
      oatmeal/NN
      is/VBZ
      not/RB
      good/JJ
      ./.
      Its/PRP$
      mushy/NN
      ,/,
      soft/JJ
      ,/,
      I/PRP
      do/VBP
      n't/RB
      like/VB
      it/PRP
      ./.
      (ORGANIZATION Quaker/NNP Oats/NNPS)
      is/VBZ
      the/DT
      way/NN
      to/TO
      go/VB
      ./.)


# Step 1. VADER ‡¶∏‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡ßç‡¶ï‡ßã‡¶∞‡¶ø‡¶Ç
VADER( Valence Aware Dictionary for Sentiment Reasoning)

‡¶è‡¶á ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ‡ßá ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï‡ßç‡¶ü‡¶æ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßá ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶™‡ßç‡¶∞‡¶ø ‡¶°‡¶ø‡¶´‡¶æ‡¶á‡¶®‡ßç‡¶° ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá‡•§ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶∏‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡¶ú‡ßá‡¶ü‡¶ø‡¶≠, ‡¶®‡ßá‡¶ó‡ßá‡¶ü‡¶ø‡¶≠ , ‡¶®‡¶ø‡¶â‡¶ü‡ßç‡¶∞‡¶æ‡¶≤ ‡¶¨‡ßá‡¶∏‡¶ø‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßã‡¶∞‡¶ø‡¶Ç ‡¶ï‡¶∞‡ßá ‡¶è‡¶á 

We will use NLTK's `SentimentIntensityAnalyzer` to get the neg/neu/pos scores of the text.

- ‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡ßç‡¶ü‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ó ‡¶Ö‡¶´ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶è‡¶™‡ßç‡¶∞‡ßã‡¶ö, ‡¶è‡¶ñ‡¶æ‡¶®‡ßá:
    1. ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶∏‡ßç‡¶ü‡¶™ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶∏‡ßç‡¶ü‡¶™ ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï and, or ,but ‡¶Ø‡¶æ‡¶§‡ßÄ‡ßü ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°
    2. ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶ï‡ßá ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶´‡¶æ‡¶á‡¶®‡¶æ‡¶≤ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá


```python
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm

sia = SentimentIntensityAnalyzer()
```


```python
sia.polarity_scores('I am so happy!')
```




    {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.6468}




```python
sia.polarity_scores('This is the worst thing ever.')
```




    {'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.6249}




```python
sia.polarity_scores(example)
```




    {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448}




```python
sia.polarity_scores(examaple2)
```




    {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.8769}




```python
# ‡¶™‡ßÅ‡¶∞‡ßã ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßá‡¶ü‡ßá ‡¶™‡ßã‡¶≤‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶®‡¶ø‡ßü‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø ‡¶è‡¶ñ‡¶®
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['Text']
    myid = row['Id']
    res[myid] = sia.polarity_scores(text)
```


      0%|          | 0/500 [00:00<?, ?it/s]



```python
vaders = pd.DataFrame(res).T
vaders = vaders.reset_index().rename(columns={'index': 'Id'})
vaders = vaders.merge(df, how='left')
```


```python
# ‡¶∏‡¶¨ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶®‡¶ø‡¶á
vaders.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>ProductId</th>
      <th>UserId</th>
      <th>ProfileName</th>
      <th>HelpfulnessNumerator</th>
      <th>HelpfulnessDenominator</th>
      <th>Score</th>
      <th>Time</th>
      <th>Summary</th>
      <th>Text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.000</td>
      <td>0.695</td>
      <td>0.305</td>
      <td>0.9441</td>
      <td>B001E4KFG0</td>
      <td>A3SGXH7AUHU8GW</td>
      <td>delmartian</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>1303862400</td>
      <td>Good Quality Dog Food</td>
      <td>I have bought several of the Vitality canned d...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.138</td>
      <td>0.862</td>
      <td>0.000</td>
      <td>-0.5664</td>
      <td>B00813GRG4</td>
      <td>A1D87F6ZCVE5NK</td>
      <td>dll pa</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1346976000</td>
      <td>Not as Advertised</td>
      <td>Product arrived labeled as Jumbo Salted Peanut...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.091</td>
      <td>0.754</td>
      <td>0.155</td>
      <td>0.8265</td>
      <td>B000LQOCH0</td>
      <td>ABXLMWJIXXAIN</td>
      <td>Natalia Corres "Natalia Corres"</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>1219017600</td>
      <td>"Delight" says it all</td>
      <td>This is a confection that has been around a fe...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>0.0000</td>
      <td>B000UA0QIQ</td>
      <td>A395BORC6FGVXV</td>
      <td>Karl</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
      <td>1307923200</td>
      <td>Cough Medicine</td>
      <td>If you are looking for the secret ingredient i...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0.000</td>
      <td>0.552</td>
      <td>0.448</td>
      <td>0.9468</td>
      <td>B006K2ZZ7K</td>
      <td>A1UQRSCLF8GW1T</td>
      <td>Michael D. Bigham "M. Wassir"</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>1350777600</td>
      <td>Great taffy</td>
      <td>Great taffy at a great price.  There was a wid...</td>
    </tr>
  </tbody>
</table>
</div>



## VADER ‡¶è‡¶∞ ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶ï‡ßá ‡¶™‡ßç‡¶≤‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø


```python
ax = sns.barplot(data=vaders, x='Score', y='compound')
ax.set_title('Compund Score by Amazon Star Review')
plt.show()
```


    
![png](output_29_0.png)
    



```python
fig, axs = plt.subplots(1, 3, figsize=(12, 3))
sns.barplot(data=vaders, x='Score', y='pos', ax=axs[0])
sns.barplot(data=vaders, x='Score', y='neu', ax=axs[1])
sns.barplot(data=vaders, x='Score', y='neg', ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()
```


    
![png](output_30_0.png)
    


# Roberta ‡¶™‡ßç‡¶∞‡¶ø ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶®‡¶° ‡¶Æ‡¶°‡ßá‡¶≤

- Use a model trained of a large corpus of data.
- Transformer model accounts for the words but also the context related to other words.


```python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
```


```python
MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
```


    config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]



    vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]



    merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]



    special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]



    pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]



```python
# VADER results on example
print(example)
sia.polarity_scores(example)
```

    This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go.





    {'neg': 0.22, 'neu': 0.78, 'pos': 0.0, 'compound': -0.5448}




```python
#  Roberta Model ‡¶∞‡¶æ‡¶® ‡¶ï‡¶∞‡¶õ‡¶ø
encoded_text = tokenizer(example, return_tensors='pt')
output = model(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_neg' : scores[0],
    'roberta_neu' : scores[1],
    'roberta_pos' : scores[2]
}
print(scores_dict)
```

    {'roberta_neg': 0.97635514, 'roberta_neu': 0.020687465, 'roberta_pos': 0.0029573692}



```python
def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict
```


```python
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text = row['Text']
        myid = row['Id']
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {}
        for key, value in vader_result.items():
            vader_result_rename[f"vader_{key}"] = value
        roberta_result = polarity_scores_roberta(text)
        both = {**vader_result_rename, **roberta_result}
        res[myid] = both
    except RuntimeError:
        print(f'Broke for id {myid}')
```


      0%|          | 0/500 [00:00<?, ?it/s]


    Broke for id 83
    Broke for id 187



```python
results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns={'index': 'Id'})
results_df = results_df.merge(df, how='left')
```

## Compare Scores between models


```python
results_df.columns
```




    Index(['Id', 'vader_neg', 'vader_neu', 'vader_pos', 'vader_compound',
           'roberta_neg', 'roberta_neu', 'roberta_pos', 'ProductId', 'UserId',
           'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator',
           'Score', 'Time', 'Summary', 'Text'],
          dtype='object')



# ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶á‡¶´‡ßá‡¶ï‡ßç‡¶ü‡¶ø‡¶≠‡¶®‡ßá‡¶∏ ‡¶ï‡¶Æ‡ßç‡¶™‡ßá‡ßü‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø


```python
sns.pairplot(data=results_df,
             vars=['vader_neg', 'vader_neu', 'vader_pos',
                  'roberta_neg', 'roberta_neu', 'roberta_pos'],
            hue='Score',
            palette='tab10')
plt.show()
```


    
![png](output_42_0.png)
    


# Review Examples:

- Positive 1-Star and Negative 5-Star Reviews

Lets look at some examples where the model scoring and review score differ the most.


```python
results_df.query('Score == 1') \
    .sort_values('roberta_pos', ascending=False)['Text'].values[0]
```




    'I felt energized within five minutes, but it lasted for about 45 minutes. I paid $3.99 for this drink. I could have just drunk a cup of coffee and saved my money.'




```python
results_df.query('Score == 1') \
    .sort_values('vader_pos', ascending=False)['Text'].values[0]
```




    'So we cancelled the order.  It was cancelled without any problem.  That is a positive note...'




```python
# nevative sentiment 5-Star view
```


```python
results_df.query('Score == 5') \
    .sort_values('roberta_neg', ascending=False)['Text'].values[0]
```




    'this was sooooo deliscious but too bad i ate em too fast and gained 2 pds! my fault'




```python
results_df.query('Score == 5') \
    .sort_values('vader_neg', ascending=False)['Text'].values[0]
```




    'this was sooooo deliscious but too bad i ate em too fast and gained 2 pds! my fault'



# ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶
